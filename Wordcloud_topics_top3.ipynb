{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4eddd3a",
   "metadata": {},
   "source": [
    "# 1. Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6687d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le modèle spaCy\n",
    "import spacy\n",
    "# spacy.cli.download(\"en_core_web_md\")\n",
    "import en_core_web_md\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851c8555",
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_fichier = 'reviews_processed_with_spacy_md.csv'\n",
    "df = pd.read_csv(chemin_fichier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c42b2",
   "metadata": {},
   "source": [
    "# 2. Définition des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570601c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dictionnaire\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction du dataframe\n",
    "col = [\"firm\",\"headline_clean\",\"pros_clean\",\"cons_clean\"]\n",
    "df_reduced = df[col].astype(str)\n",
    "list_firms = df_reduced[\"firm\"].unique()\n",
    "# list_firms = [\"Apple\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation des fonctions\n",
    "   \n",
    "def process_wordcloud(df, custom_stopwords={}):\n",
    "    \"\"\"\n",
    "    Fonction calculant et retournant le nuage de mots.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(nlp.Defaults.stop_words) # Utilisez un ensemble pour une recherche plus rapide\n",
    "\n",
    "    if custom_stopwords:\n",
    "        if isinstance(custom_stopwords, list):\n",
    "            stop_words.update(custom_stopwords) # Ajouter les mots à l'ensemble\n",
    "        else: #Si ce n'est pas une liste, on suppose que c'est déjà un set.\n",
    "            stop_words.update(custom_stopwords)\n",
    "\n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(max_words = 30, width=1000, height=600, background_color='white', stopwords=stop_words).generate(' '.join(df.fillna('').astype(str)))\n",
    "\n",
    "\n",
    "    return wordcloud\n",
    "\n",
    "def display_wordcloud(wordcloud, main_subject_title):\n",
    "    \"\"\"\n",
    "    Fonction affichant le nuage de mots.\n",
    "    \"\"\"\n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Most Frequent Words in {main_subject_title} Reviews', fontsize=16, pad=40)\n",
    "    plt.show()\n",
    "\n",
    "def most_common_words_identification(df, nb_words=30):\n",
    "    \"\"\"\n",
    "    Fonction calculant et retournant les mots les plus fréquents.\n",
    "    \"\"\"\n",
    "    # Combine all pros texts into a single string\n",
    "    text = ' '.join(df)\n",
    "\n",
    "    # Split to wordsand count frequency\n",
    "    word_counts = Counter(text.split())\n",
    "\n",
    "    # Nombre of words\n",
    "    total_word = len(text.split())\n",
    "\n",
    "    # Checking top-nb_words\n",
    "    common_words = word_counts.most_common(nb_words)\n",
    "    # for word, count in common_words:\n",
    "    #     print(f\"{word}: {count} ({count / total_word:.2%})\")\n",
    "    return common_words\n",
    "\n",
    "\n",
    "# Vectorize the pros column\n",
    "def generate_vectorizer(df, n_gram_range=(1, 2), custom_stop_word={}):\n",
    "    \"\"\"\n",
    "    Fonction pour générer le TF-IDF.\n",
    "    \"\"\"\n",
    "    text = []\n",
    "\n",
    "    stop_words = set(nlp.Defaults.stop_words) # Utilisez un ensemble pour une recherche plus rapide\n",
    "\n",
    "    if custom_stop_word:\n",
    "        if isinstance(custom_stop_word, list):\n",
    "            stop_words.update(custom_stop_word) # Ajouter les mots à l'ensemble\n",
    "        else: #Si ce n'est pas une liste, on suppose que c'est déjà un set.\n",
    "            stop_words.update(custom_stop_word)\n",
    "\n",
    "    for sentence in df:\n",
    "        t = ' '.join([word for word in sentence.split() if word not in stop_words])\n",
    "        text.append(t)\n",
    "\n",
    "    vectorizer_TFIDF = TfidfVectorizer(max_features=1000, ngram_range=n_gram_range, stop_words='english', min_df = 10, max_df = 0.9)\n",
    "    vectorizer_Count = CountVectorizer(max_features=1000, ngram_range=n_gram_range, stop_words='english', min_df = 10, max_df = 0.9)\n",
    "\n",
    "    X_Tfidf = vectorizer_TFIDF.fit_transform(text)\n",
    "    X_Count = vectorizer_Count.fit_transform(text)\n",
    "    return X_Tfidf, X_Count, vectorizer_TFIDF,vectorizer_Count\n",
    "\n",
    "def display_top_topics(vectorizer, model, n_top_words=10):\n",
    "    \"\"\"\n",
    "    Fonction pour afficher les topics les plus fréquents.\n",
    "    \"\"\"\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    # print(\"Terms:\\n\")\n",
    "    # print(terms)\n",
    "    for i, comp in enumerate(model.components_):\n",
    "        terms_in_topic = [terms[j] for j in comp.argsort()[:-(n_top_words+1):-1]]\n",
    "        print(f\"Topic {i+1}: {' | '.join(terms_in_topic)}\")\n",
    "\n",
    "# SVD decomposition\n",
    "def svd_decomposition(X, n_components=2):\n",
    "    \"\"\"\n",
    "    Fonction pour effectuer la décomposition tronquée SVD.\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    # Fit the model and transform the data\n",
    "    lsa = svd.fit(X)\n",
    "    return lsa\n",
    "\n",
    "def lda_decomposition(X,n_components=2):\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, random_state=42)\n",
    "    return lda.fit(X)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fonctions pour le traitement des lemmes (sous forme de chaînes) ---\n",
    "\n",
    "def get_embedding_from_lemmas_string(lemmas_string, nlp_model):\n",
    "    \"\"\"\n",
    "    Calcule l'embedding moyen à partir d'une chaîne de caractères contenant des lemmes séparés par des espaces.\n",
    "    \"\"\"\n",
    "    lemmas_list = lemmas_string.split() # Fendre la chaîne en une liste de lemmes\n",
    "    vectors = []\n",
    "    for lemma in lemmas_list:\n",
    "        # Vérifiez si le lemme est dans le vocabulaire du modèle et a un vecteur\n",
    "        if lemma in nlp_model.vocab and nlp_model.vocab[lemma].has_vector:\n",
    "            vectors.append(nlp_model.vocab[lemma].vector)\n",
    "\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # Retourne un vecteur de zéros si aucun lemme n'est trouvé ou n'a de vecteur\n",
    "        return np.zeros(nlp_model.vocab.vectors.shape[1])\n",
    "\n",
    "\n",
    "def process_lemmas_strings_in_batches(data_input_series, batch_size, nlp_model):\n",
    "    \"\"\"\n",
    "    Fonction principale pour traiter une Series de chaînes de lemmes par lots et générer les embeddings.\n",
    "\n",
    "    Args:\n",
    "        data_input_series (pd.Series): Une Series pandas où chaque entrée est une chaîne de lemmes\n",
    "                                       séparés par des espaces.\n",
    "                                       Ex: pd.Series([\"flexibilité horaire\", \"télétravail encourager\"])\n",
    "        batch_size (int): La taille des lots pour le traitement.\n",
    "        nlp_model: L'instance du modèle spaCy chargé.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Un tuple contenant (document_embeddings, original_statements_for_display)\n",
    "    \"\"\"\n",
    "    document_embeddings = []\n",
    "    original_statements_for_display = [] # Pour conserver la chaîne originale pour l'affichage\n",
    "\n",
    "    if not isinstance(data_input_series, pd.Series):\n",
    "        raise TypeError(\"data_input_series doit être une Series pandas.\")\n",
    "\n",
    "    # print(f\"Génération des embeddings par lots (taille de lot : {batch_size})...\")\n",
    "\n",
    "    # Itérer sur les indices pour créer des lots à partir de la Series\n",
    "    for i in range(0, len(data_input_series), batch_size):\n",
    "        batch_of_strings = data_input_series.iloc[i:i + batch_size]\n",
    "\n",
    "        for lemmas_string in batch_of_strings: # Chaque 'lemmas_string' est une chaîne d'un avantage\n",
    "            # Vérifiez si la chaîne n'est pas vide (peut arriver après le nettoyage)\n",
    "            if pd.isna(lemmas_string) or not lemmas_string.strip():\n",
    "                embedding = np.zeros(nlp_model.vocab.vectors.shape[1])\n",
    "            else:\n",
    "                embedding = get_embedding_from_lemmas_string(str(lemmas_string), nlp_model) # Assurez-vous que c'est une str\n",
    "\n",
    "            document_embeddings.append(embedding)\n",
    "            original_statements_for_display.append(str(lemmas_string)) # Conserver la chaîne originale\n",
    "\n",
    "    document_embeddings = np.array(document_embeddings)\n",
    "\n",
    "    # print(f\"Forme des embeddings de documents : {document_embeddings.shape}\")\n",
    "\n",
    "\n",
    "    return document_embeddings, original_statements_for_display\n",
    "\n",
    "\n",
    "\n",
    "def k_means_algorithm(df,nb_clusters = 5):\n",
    "    # --- Exécution du pipeline avec la Series de lemmes ---\n",
    "    batch_size_for_processing = 1000\n",
    "\n",
    "    document_embeddings, original_statements_for_display = process_lemmas_strings_in_batches(\n",
    "        df, batch_size_for_processing, nlp\n",
    "    )\n",
    "\n",
    "    ## Application de l'algorithme K-Means\n",
    "    n_clusters = nb_clusters # Nombre de catégories \n",
    "\n",
    "    if n_clusters > len(document_embeddings):\n",
    "        n_clusters = len(document_embeddings)\n",
    "        print(f\"Attention : Le nombre de clusters a été ajusté à {n_clusters} car il y a moins de documents.\")\n",
    "\n",
    "    print(f\"\\nExécution de K-Means avec {n_clusters} clusters...\")\n",
    "    kmeans_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans_model.fit_predict(document_embeddings)\n",
    "    print(\"Clustering K-Means terminé.\")\n",
    "\n",
    "    ## Interprétation des clusters\n",
    "    print(\"\\n--- Échantillon de chaque cluster pour interprétation ---\")\n",
    "    df_results = pd.DataFrame({\n",
    "        'statement': original_statements_for_display,\n",
    "        'cluster': clusters,\n",
    "        'embedding': list(document_embeddings)\n",
    "    })\n",
    "\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_sample = df_results[df_results['cluster'] == cluster_id].sample(min(10, len(df_results[df_results['cluster'] == cluster_id])))\n",
    "        print(f\"Cluster {cluster_id} ({len(df_results[df_results['cluster'] == cluster_id])})\")\n",
    "        # for i, row in cluster_sample.iterrows():\n",
    "        #     print(f\"  - {row['statement']}\")\n",
    "\n",
    "    ## Visualisation des clusters (avec t-SNE)\n",
    "    # t-SNE : algorithme de réduction de dimensionnalité non linéaire \n",
    "    # qui est particulièrement bien adapté à la visualisation de données de haute dimension (comme ici avec la taille des pros et cons)\n",
    "    sample_size_for_tsne = min(2000, len(document_embeddings))\n",
    "    sample_indices = random.sample(range(len(document_embeddings)), sample_size_for_tsne)\n",
    "    sampled_embeddings = document_embeddings[sample_indices]\n",
    "    sampled_clusters = clusters[sample_indices]\n",
    "    sampled_statements = [original_statements_for_display[i] for i in sample_indices]\n",
    "\n",
    "    if len(sampled_embeddings) > 5:\n",
    "        print(f\"\\nRéduction de dimension avec t-SNE sur {len(sampled_embeddings)} échantillons (peut prendre du temps)...\")\n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(sampled_embeddings) - 1), n_iter=1000)\n",
    "        reduced_embeddings = tsne.fit_transform(sampled_embeddings)\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.scatterplot(\n",
    "            x=reduced_embeddings[:, 0],\n",
    "            y=reduced_embeddings[:, 1],\n",
    "            hue=sampled_clusters,\n",
    "            palette=sns.color_palette(\"hsv\", n_clusters),\n",
    "            legend=\"full\",\n",
    "            s=80,\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "        for i in random.sample(range(len(sampled_embeddings)), min(50, len(sampled_embeddings))):\n",
    "            plt.text(reduced_embeddings[i, 0] + 0.05, reduced_embeddings[i, 1] + 0.05,\n",
    "                    f\"{i}: {sampled_statements[i][:20]}...\", fontsize=6, alpha=0.6)\n",
    "\n",
    "        plt.title(f\"Visualisation des clusters (t-SNE - Échantillon de {len(sampled_embeddings)})\")\n",
    "        plt.xlabel(\"Composante t-SNE 1\")\n",
    "        plt.ylabel(\"Composante t-SNE 2\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nPas assez de documents pour la visualisation t-SNE de l'échantillon.\")\n",
    "\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fc817",
   "metadata": {},
   "source": [
    "# 3. Traitements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffa7a00",
   "metadata": {},
   "source": [
    "## 3.1 Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90aefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common word for the whole dataset\n",
    "nb_top_words = 10\n",
    "common_word_pros = most_common_words_identification(df_reduced[\"pros_clean\"], nb_top_words)\n",
    "common_word_cons = most_common_words_identification(df_reduced[\"cons_clean\"], nb_top_words)\n",
    "common_word_headline = most_common_words_identification(df_reduced[\"headline_clean\"], nb_top_words)\n",
    "common_word = common_word_pros+common_word_cons+common_word_headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde830e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des wordclouds\n",
    "for firm in list_firms:\n",
    "  df_firm = df_reduced[df_reduced[\"firm\"] == firm].astype(str)\n",
    "  print(f\"Firm: {firm}\\n\")\n",
    "\n",
    "  for c in [\"pros\",\"cons\",\"headline\"]:\n",
    "      print(c.upper())\n",
    "      custom_stop_word = {firm.lower()}\n",
    "      custom_stop_word.update({word for word,count in common_word})\n",
    "      if c == \"pros\":\n",
    "         col = \"pros_clean\"\n",
    "         custom_stop_word.update({word for word,count in common_word_pros})\n",
    "      if c == \"cons\":\n",
    "         col = \"cons_clean\"\n",
    "         custom_stop_word.update({word for word,count in common_word_cons})\n",
    "      if c == \"headline\":\n",
    "         col = \"headline_clean\"\n",
    "         custom_stop_word.update({word for word,count in common_word_headline})\n",
    "      \n",
    "      # Common words identification\n",
    "      common_words = most_common_words_identification(df_firm[f\"{col}\"], 10)\n",
    "      custom_stop_word.update({word for word,count in common_words})\n",
    "\n",
    "            \n",
    "      # WORDCLOUD\n",
    "      wd = process_wordcloud(df_firm[f\"{col}\"], custom_stopwords=custom_stop_word)    \n",
    "      if c == \"headline\":\n",
    "        display_wordcloud(wd, f\"{firm}: Headline\")\n",
    "      elif c == \"pros\":\n",
    "        display_wordcloud(wd, f\"{firm}: Pros\")\n",
    "      elif c == \"cons\":\n",
    "        display_wordcloud(wd, f\"{firm}: Cons\")\n",
    "      print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e215ac4",
   "metadata": {},
   "source": [
    "## 3.2 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Affichage des topics\n",
    "for firm in list_firms:\n",
    "  df_firm = df_reduced[df_reduced[\"firm\"] == firm].astype(str)\n",
    "  print(f\"Firm: {firm}\\n\")\n",
    "  \n",
    "\n",
    "#   for c in [\"pros\"]:\n",
    "  for c in [\"pros\",\"cons\",\"headline\"]:\n",
    "      print(c.upper())\n",
    "      custom_stop_word = {firm.lower()}\n",
    "      if c == \"pros\":\n",
    "         col = \"pros_clean\"\n",
    "         custom_stop_word.update({word for word,count in common_word_pros})\n",
    "      if c == \"cons\":\n",
    "         col = \"cons_clean\"\n",
    "         custom_stop_word.update({word for word,count in common_word_cons})\n",
    "      if c == \"headline\":\n",
    "         col = \"headline_clean\"\n",
    "         custom_stop_word.update({word for word,count in common_word_headline})\n",
    "      \n",
    "      # Common words identification\n",
    "      common_words = most_common_words_identification(df_firm[f\"{col}\"], 10)\n",
    "      custom_stop_word.update({word for word,count in common_words})\n",
    "\n",
    "      # TOPICS\n",
    "      # Generate TF-IDF matrix\n",
    "      X_Tfidf ,X_Count, vectorizer_TFIDF, vectorizer_Count = generate_vectorizer(df_firm[f\"{col}\"],\n",
    "                                                                               n_gram_range=(2, 3),\n",
    "                                                                               custom_stop_word=custom_stop_word)\n",
    "      \n",
    "      nb_topics = 3\n",
    "      # Perform LSA\n",
    "      lsa = svd_decomposition(X_Tfidf, n_components=nb_topics)\n",
    "      # Display top topics\n",
    "      print(f\"Top topics from LSA for {firm} in {c}:\")\n",
    "      display_top_topics(vectorizer_TFIDF, lsa, n_top_words=5)\n",
    "      print()\n",
    "\n",
    "      # # Perform LDA\n",
    "      # lda = lda_decomposition(X_Count, n_components=nb_topics)\n",
    "      # print(f\"Top topics from LDA for {firm} in {c}:\")\n",
    "      # display_top_topics(vectorizer_Count, lda, n_top_words=5)\n",
    "      # print()\n",
    "\n",
    "      feature_names = vectorizer_Count.get_feature_names_out()\n",
    "      word_counts = np.asarray(X_Count.sum(axis=0)).flatten() # Convertir la matrice somme en un array 1D\n",
    "\n",
    "      # Créer un DataFrame Pandas pour faciliter le tri\n",
    "      words_df = pd.DataFrame({'term': feature_names, 'count': word_counts})\n",
    "\n",
    "      # 3.3. Trier par fréquence et sélectionner le top 3\n",
    "      top_words = words_df.sort_values(by='count', ascending=False).reset_index().head(3)\n",
    "      top_words = top_words.drop('index', axis=1)\n",
    "      print(\"\\n--- Top 3 des mots/n-grammes les plus utilisés dans le corpus ---\")\n",
    "      print(top_words)\n",
    "      print(\"*********************\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6977a50",
   "metadata": {},
   "source": [
    "## 3.3 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62376ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "nb_cluster = 9\n",
    "\n",
    "for firm in list_firms:\n",
    "    df_firm = df[df[\"firm\"] == firm].astype(str)\n",
    "    print(f\"Firm: {firm}\\n\")\n",
    "\n",
    "    # Étape 1: Remplacer les NaN par des chaînes vides pour éviter les erreurs lors du split\n",
    "    # et s'assurer que c'est bien une chaîne de caractères\n",
    "    df_firm['pros_processed'] = df_firm['pros_clean'].astype(str).replace('nan', '')\n",
    "\n",
    "    # Étape 2: Diviser la chaîne par le point ('.') et la virgule (',').\n",
    "    df_firm['pros_processed'] = df_firm['pros_processed'].apply(\n",
    "        lambda x: [s.strip() for s in re.split(r'[.,]', x) if s.strip()]\n",
    "    )\n",
    "\n",
    "    # Étape 3: Utiliser .explode() pour transformer chaque élément de la liste en une nouvelle ligne.\n",
    "    # Cela va dupliquer les autres colonnes (comme 'id_employee') pour chaque nouvelle entrée.\n",
    "    df_expanded = df_firm.explode('pros_processed')\n",
    "\n",
    "    # Étape 4: Nettoyer les lignes résultantes qui pourraient être vides\n",
    "    df_expanded = df_expanded[df_expanded['pros_processed'] != ''].reset_index(drop=True)      \n",
    "\n",
    "    # Étape 5: Application du clustering\n",
    "    df_result = k_means_algorithm(df_expanded[\"pros_processed\"], nb_cluster)\n",
    "      \n",
    "    most_common_words_identification(df_result[df_result[\"cluster\"] == 0], 10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = most_common_words_identification(df_result[df_result[\"cluster\"] == 0], 10)\n",
    "# result\n",
    "df_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
